{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Doc2Vecの動作確認（bot_id=11）\n",
    "\n",
    "\n",
    "実行条件は下記の通りとします。\n",
    "\n",
    "- すべての教師データのタグをユニークにするため、[ラベル]\\_[連番] 形式のタグを付与\n",
    "\n",
    "\n",
    "- 品詞を落とさないようにする\n",
    "\n",
    "\n",
    "- DBOWを使用\n",
    "\n",
    "\n",
    "- feature数は、実質的な教師データのパターン数と整合させる（ユーザーさんによっては、回答ラベル数が必ずしも回答パターン数と合致しないため、ラベル数の２倍程度の値を設定するのを目安とします）\n",
    "\n",
    "\n",
    "- 学習時の反復回数を [feature数 * 10] に設定\n",
    "\n",
    "\n",
    "現在テストデータとして用意されているBot（ID=11：benefitone.csv）で動作確認を行います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## (1) テストデータ／環境準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    テスト環境を準備するためのモジュールを使用します。\n",
    "'''\n",
    "import sys\n",
    "import os\n",
    "learning_dir = os.path.abspath(\"../../\") #<--- donusagi-bot/learning\n",
    "os.chdir(learning_dir)\n",
    "\n",
    "if learning_dir not in sys.path:\n",
    "    sys.path.append(learning_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## (2) 学習／予測処理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### (2-1) コーパス生成\n",
    "\n",
    "コーパス（単語が半角スペースで区切られた文字列）生成時、一部の品詞を落とすようにします。\n",
    "\n",
    "（＝learning.core.nlang.Nlang クラスの仕様に従います）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from learning.core.learn.learning_parameter import LearningParameter\n",
    "from learning.core.datasource import Datasource\n",
    "\n",
    "_bot_id = 11\n",
    "attr = {\n",
    "    'include_failed_data': False,\n",
    "    'include_tag_vector': False,\n",
    "    'classify_threshold': 0.5,\n",
    "    'algorithm': LearningParameter.ALGORITHM_LOGISTIC_REGRESSION,\n",
    "    'params_for_algorithm': {'C': 140},\n",
    "    'excluded_labels_for_fitting': None\n",
    "}\n",
    "\n",
    "learning_parameter = LearningParameter(attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017/05/19 PM 03:42:17 ['./fixtures/learning_training_messages/benefitone.csv', './fixtures/learning_training_messages/ptna.csv', './fixtures/learning_training_messages/septeni.csv', './fixtures/learning_training_messages/toyotsu_human.csv']\n",
      "2017/05/19 PM 03:42:18 ['./fixtures/question_answers/toyotsu_human.csv']\n"
     ]
    }
   ],
   "source": [
    "_datasource = Datasource(type='csv')\n",
    "learning_training_messages = _datasource.learning_training_messages(_bot_id)\n",
    "questions = np.array(learning_training_messages['question'])\n",
    "answer_ids = np.array(learning_training_messages['answer_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import MeCab\n",
    "import mojimoji\n",
    "\n",
    "class Nlang_naive:\n",
    "    @classmethod\n",
    "    def split(self, text):\n",
    "        tagger = MeCab.Tagger(\"-u learning/dict/custom.dic\")\n",
    "        tagger.parse('')  # node.surfaceを取得出来るようにするため、空文字をparseする(Python3のバグの模様)\n",
    "        node = tagger.parseToNode(text)\n",
    "        word_list = []\n",
    "        while node:\n",
    "            features = node.feature.split(\",\")\n",
    "            pos = features[0]\n",
    "            if pos in [\"BOS/EOS\", \"記号\"]:\n",
    "                node = node.next\n",
    "                continue\n",
    "\n",
    "            #print(features)\n",
    "            lemma = node.feature.split(\",\")[6]\n",
    "\n",
    "            if lemma == \"*\":\n",
    "                lemma = node.surface  #.decode(\"utf-8\")\n",
    "                \n",
    "            word_list.append(mojimoji.han_to_zen(lemma))\n",
    "            node = node.next\n",
    "        return \" \".join(word_list)\n",
    "\n",
    "    @classmethod\n",
    "    def batch_split(self, texts):\n",
    "        splited_texts = []\n",
    "        for text in texts:\n",
    "            splited_texts.append(self.split(text))\n",
    "        return splited_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from learning.core.nlang import Nlang\n",
    "\n",
    "_sentences = np.array(questions)\n",
    "_separated_sentences = Nlang_naive.batch_split(_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### (2-2) タグ付け\n",
    "\n",
    "ラベル毎の教師データ数を制限した学習セットを構築します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "class Doc2VecTrainingSet:\n",
    "    def __init__(self):\n",
    "        self.answer_id_count = {}\n",
    "        self.selected_answer_ids = []\n",
    "        self.selected_questions = []\n",
    "        self.selected_separated_sentences = []\n",
    "\n",
    "    def build(self, questions, answer_ids, separated_sentences):\n",
    "        for index, answer_id in enumerate(answer_ids):\n",
    "            if answer_id not in self.answer_id_count.keys():\n",
    "                self.answer_id_count[answer_id] = 0\n",
    "\n",
    "            '''\n",
    "                データをリストに格納\n",
    "                ラベルはユニークにする必要があるので、\n",
    "                [answer_id]_[連番] の形式で編集\n",
    "            '''\n",
    "            self.selected_answer_ids.append('%04d_%02d' % (\n",
    "                answer_id,\n",
    "                self.answer_id_count[answer_id]\n",
    "            ))\n",
    "            self.selected_questions.append(questions[index])\n",
    "            self.selected_separated_sentences.append(separated_sentences[index])\n",
    "            self.answer_id_count[answer_id] += 1\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def get_tagged_document_list(self):\n",
    "        '''\n",
    "            ユニークになったコーパス／ラベルから、\n",
    "            学習セットを生成する\n",
    "        '''\n",
    "        tagged_document_list = self.__corpus_to_sentences(\n",
    "            self.selected_separated_sentences, \n",
    "            self.selected_answer_ids\n",
    "        )\n",
    "\n",
    "        return tagged_document_list\n",
    "\n",
    "    def __get_tagged_document(self, sentences, name):\n",
    "        '''\n",
    "            models.doc2vecの仕様に従い\n",
    "            コーパスにタグ付け\n",
    "        '''\n",
    "        words = sentences.split(' ')\n",
    "        return TaggedDocument(words=words, tags=[name])\n",
    "\n",
    "    def __corpus_to_sentences(self, separated_sentences, answer_ids):\n",
    "        '''\n",
    "            TaggedDocumentを生成し、リストに格納\n",
    "        '''\n",
    "        tagged_document_list = []\n",
    "        for idx, (doc, name) in enumerate(zip(separated_sentences, answer_ids)):\n",
    "            tagged_document = self.__get_tagged_document(doc, name)\n",
    "            tagged_document_list.append(tagged_document)\n",
    "            \n",
    "        return tagged_document_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Doc2VecTrainingSet at 0x104382278>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    ユニークになったコーパス／ラベルから、\n",
    "    学習セットを生成する\n",
    "    \n",
    "    ラベル数＝88、サンプル数＝7,083\n",
    "'''\n",
    "d2v_training_set = Doc2VecTrainingSet()\n",
    "d2v_training_set.build(questions, answer_ids, _separated_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### (2-3) 学習処理／モデルのシリアライズ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def doc2vec_model_path(bot_id):\n",
    "    model_path = 'prototype/better_algorithm/doc2vec.bot%02d.model' % bot_id\n",
    "\n",
    "    return model_path\n",
    "\n",
    "def train_by_doc2vec(bot_id, doc2vec_training_set):\n",
    "    '''\n",
    "        パラメータ：\n",
    "         学習時にDBOWを使用する\n",
    "         featureの数を回答パターン数の２倍（仮決め）に設定\n",
    "         反復回数をfeature数の10倍に設定\n",
    "    '''\n",
    "    sentence_list = doc2vec_training_set.get_tagged_document_list()\n",
    "    n_pattern = len(doc2vec_training_set.answer_id_count)\n",
    "    print('train_by_doc2vec: Train data sample=%d, Train data pattern=%d' % (len(sentence_list), n_pattern))\n",
    "\n",
    "    n_feature = n_pattern * 2\n",
    "    n_iter = n_feature * 10\n",
    "    print('train_by_doc2vec: Feature size=%d, Max iteration count=%d' % (n_feature, n_iter))\n",
    "\n",
    "    # ボキャブラリ生成／学習実行\n",
    "    model = Doc2Vec(dm=0, size=n_feature, min_count=1, iter=n_iter)\n",
    "    model.build_vocab(sentence_list)\n",
    "    ret = model.train(sentence_list)\n",
    "\n",
    "    '''\n",
    "        モデル内に保持されているベクトルの数を取得\n",
    "        （featureの数 [回答パターン数の２倍] と同じであることを確認）\n",
    "    '''\n",
    "    if len(model.docvecs) != len(sentence_list):\n",
    "        raise Exception('train_by_doc2vec: Failed to create document vector')\n",
    "\n",
    "    # 学習モデルは、ファイルに保存しておく\n",
    "    model.save(doc2vec_model_path(bot_id))\n",
    "    print('train_by_doc2vec: document vector size=%d, return=%d' % (len(model.docvecs), ret))\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_by_doc2vec: Train data sample=7083, Train data pattern=88\n",
      "train_by_doc2vec: Feature size=176, Max iteration count=1760\n",
      "train_by_doc2vec: document vector size=7083, return=60769938\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "60769938"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    生成された学習セット（タグ付きドキュメント）を\n",
    "    使用し、学習実行\n",
    "'''\n",
    "train_by_doc2vec(_bot_id, d2v_training_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### (2-4) 予測処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def predict(word, bot_id):\n",
    "    '''\n",
    "        予測処理にかけるコーパスを生成\n",
    "        （学習セット作成時と同じ関数を使用）\n",
    "    '''\n",
    "    corpus = Nlang_naive.split(word).split()\n",
    "\n",
    "    '''\n",
    "        コーパスからベクトルを生成し、\n",
    "        ロードしたモデルから類似ベクトルを検索\n",
    "    '''\n",
    "    loaded_model = models.Doc2Vec.load(doc2vec_model_path(bot_id))\n",
    "    inferred_vector = loaded_model.infer_vector(corpus)\n",
    "    ret = loaded_model.docvecs.most_similar([inferred_vector])\n",
    "\n",
    "    return corpus, ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['契約', '書', 'を', '見る', 'たい', 'の', 'です', 'が'],\n",
       " [('4683_24', 0.8125563263893127),\n",
       "  ('4683_12', 0.8092948794364929),\n",
       "  ('4683_25', 0.7589855194091797),\n",
       "  ('4683_00', 0.7501294612884521),\n",
       "  ('4683_21', 0.7027206420898438),\n",
       "  ('4683_09', 0.6920285224914551),\n",
       "  ('4683_15', 0.6821205019950867),\n",
       "  ('4683_03', 0.6742035746574402),\n",
       "  ('4683_23', 0.6493698358535767),\n",
       "  ('4683_11', 0.6471184492111206)])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    契約書を見たいのですが（正解＝4683）\n",
    "'''\n",
    "predict('契約書を見たいのですが', _bot_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['ＥＸ', 'カード', 'を', '貸す', 'て', 'くださる'],\n",
       " [('4678_2700', 0.8527379035949707),\n",
       "  ('4678_692', 0.8525393009185791),\n",
       "  ('4678_2694', 0.7479832172393799),\n",
       "  ('4678_686', 0.7462894320487976),\n",
       "  ('4727_11', 0.7335948944091797),\n",
       "  ('4727_05', 0.7287341356277466),\n",
       "  ('4678_4220', 0.7135286331176758),\n",
       "  ('4742_10', 0.7053501605987549),\n",
       "  ('4678_2218', 0.7035999894142151),\n",
       "  ('4678_4226', 0.7026973962783813)])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    EXカードを貸してください（正解＝4678）\n",
    "'''\n",
    "predict('EXカードを貸してください', _bot_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## (3) accuracy 測定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def predict_similarity(separated_sentence):\n",
    "    corpus = separated_sentence.split()\n",
    "    inferred_vector = loaded_model.infer_vector(corpus)\n",
    "    ret = loaded_model.docvecs.most_similar([inferred_vector])\n",
    "\n",
    "    answer_id, similarity = ret[0]\n",
    "    return corpus, answer_id, similarity\n",
    "\n",
    "def get_prediction_statistics(separated_sentences, answer_ids):\n",
    "    '''\n",
    "        学習セットの質問文をそのまま予測処理にかけて、\n",
    "        回答を予測\n",
    "    '''\n",
    "    statistics = []\n",
    "    for i, _ in enumerate(separated_sentences):\n",
    "        sentence = separated_sentences[i]\n",
    "        preferred_answer_id = answer_ids[i]\n",
    "        corpus, answer_id, similarity = predict_similarity(separated_sentences[i])\n",
    "        corpus_len = len(corpus)\n",
    "        statistics.append((i, corpus_len, preferred_answer_id, answer_id, similarity))\n",
    "\n",
    "    return statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(prediction_statistics):\n",
    "    '''\n",
    "        予測結果を、質問文の単語数毎／回答ID毎に統計する\n",
    "    '''\n",
    "    ncorrect_by_corpus_len = {}\n",
    "    nsample_by_corpus_len = {}\n",
    "\n",
    "    ncorrect = 0\n",
    "    nsample = 0\n",
    "\n",
    "    for statistics in prediction_statistics:\n",
    "        i, corpus_len, preferred_answer_id, answer_id, similarity = statistics\n",
    "\n",
    "        '''\n",
    "            質問文の単語数ごとに統計を取る\n",
    "        '''\n",
    "        if corpus_len not in nsample_by_corpus_len.keys():\n",
    "            ncorrect_by_corpus_len[corpus_len] = 0\n",
    "            nsample_by_corpus_len[corpus_len] = 0\n",
    "        nsample_by_corpus_len[corpus_len] += 1\n",
    "\n",
    "        '''\n",
    "            正解かどうか検査\n",
    "            （NNNN_nn 形式ラベルの上４桁が一致していれば正解とします）\n",
    "        '''\n",
    "        nsample += 1\n",
    "        if preferred_answer_id[0:4] == answer_id[0:4]:\n",
    "            ncorrect += 1\n",
    "            ncorrect_by_corpus_len[corpus_len] += 1\n",
    "    \n",
    "    '''\n",
    "        全体の正解率\n",
    "    '''\n",
    "    accuracy = ncorrect / nsample\n",
    "    print(\"accuracy=%0.3f (%d/%d)\" % (accuracy, ncorrect, nsample))\n",
    "\n",
    "    '''\n",
    "        質問文の単語数ごとの統計情報を編集\n",
    "    '''\n",
    "    info_by_corpus_len = []\n",
    "    for k, v in ncorrect_by_corpus_len.items():\n",
    "        info_by_corpus_len.append((\n",
    "            k, \n",
    "            ncorrect_by_corpus_len[k]/nsample_by_corpus_len[k], \n",
    "            ncorrect_by_corpus_len[k], \n",
    "            nsample_by_corpus_len[k]\n",
    "        ))\n",
    "\n",
    "    '''\n",
    "        質問文の単語数ごとの正解率をリスト\n",
    "    '''\n",
    "    print(\"Accuracy info by word count...\")\n",
    "    for info in info_by_corpus_len:\n",
    "        print(\"word_count=%2d: accuracy=%0.3f (%d/%d)\" % (\n",
    "            info[0], info[1], info[2], info[3]\n",
    "        ))\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loaded_model = models.Doc2Vec.load(doc2vec_model_path(_bot_id))\n",
    "\n",
    "selected_separated_sentences = d2v_training_set.selected_separated_sentences\n",
    "selected_answer_ids = d2v_training_set.selected_answer_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy=0.968 (6859/7083)\n",
      "Accuracy info by word count...\n",
      "word_count= 1: accuracy=0.375 (3/8)\n",
      "word_count= 2: accuracy=1.000 (40/40)\n",
      "word_count= 3: accuracy=0.987 (300/304)\n",
      "word_count= 4: accuracy=0.943 (738/783)\n",
      "word_count= 5: accuracy=0.968 (1351/1396)\n",
      "word_count= 6: accuracy=0.971 (1183/1218)\n",
      "word_count= 7: accuracy=0.967 (1120/1158)\n",
      "word_count= 8: accuracy=0.972 (811/834)\n",
      "word_count= 9: accuracy=0.984 (506/514)\n",
      "word_count=10: accuracy=0.981 (257/262)\n",
      "word_count=11: accuracy=0.966 (201/208)\n",
      "word_count=12: accuracy=0.986 (136/138)\n",
      "word_count=13: accuracy=0.968 (91/94)\n",
      "word_count=14: accuracy=0.952 (59/62)\n",
      "word_count=15: accuracy=1.000 (28/28)\n",
      "word_count=16: accuracy=0.929 (13/14)\n",
      "word_count=17: accuracy=1.000 (12/12)\n",
      "word_count=18: accuracy=1.000 (6/6)\n",
      "word_count=19: accuracy=1.000 (2/2)\n",
      "word_count=23: accuracy=1.000 (2/2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9683749823521107"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_statistics = get_prediction_statistics(selected_separated_sentences, selected_answer_ids)\n",
    "calculate_accuracy(prediction_statistics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
