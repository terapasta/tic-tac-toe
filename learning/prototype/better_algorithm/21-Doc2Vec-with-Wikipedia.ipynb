{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Doc2VecでWikipedia全文を使用\n",
    "\n",
    "Wiki全文（か、もしくは一部）をDoc2Vecのインプットに指定して動作確認します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## (1) Wikipediaコンテンツファイルから本文を抽出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### (1-1) Wikipediaコンテンツ取得\n",
    "\n",
    "Wikipedia コンテンツの一覧を、以下のURLで参照します。\n",
    "\n",
    "https://dumps.wikimedia.org/jawiki/latest/\n",
    "\n",
    "このページのリストから、以下のファイルのリンクをクリックしてダウンロードします。\n",
    "\n",
    "2.4GBほどになります。\n",
    "\n",
    "```\n",
    "jawiki-latest-pages-articles.xml.bz2    21-May-2017 01:07    2492605537\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### (1-2) Wikipedia Extractor（本文抽出ツール）の取得\n",
    "\n",
    "下記サイト内のDownloadsからWikipedia ExtractorのPythonスクリプトを直接ダウンロードします。\n",
    "\n",
    "http://medialab.di.unipi.it/wiki/Wikipedia_Extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### (1-3) Wikipediaコンテンツファイルから本文を抽出\n",
    "\n",
    "下記コマンドで抽出を実行します。\n",
    "\n",
    "python3 WikiExtractor.py -b 10M -o extracted jawiki-latest-pages-articles.xml.bz2\n",
    "\n",
    "これで、extractedディレクトリ下に、10MB毎に区切られた抽出後ファイルが生成されていきます。\n",
    "\n",
    "extractedディレクトリ配下には、AA、AB、AC・・・といったサブディレクトリが生成されます。\n",
    "\n",
    "サブディレクトリごとに、１００本のファイルが生成されます。\n",
    "\n",
    "（すなわちサブディレクトリごとに1GBまで）\n",
    "\n",
    "実行には４０分ほどかかるとのことでしたが、実績としては、5/22 13:51開始〜14:26終了、なので３５分ほどで完了しています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "実行例：\n",
    "```\n",
    "MacBookPro-makmorit-jp:Wikipedia makmorit$ pwd\n",
    "/Users/makmorit/Documents/Development/Wikipedia\n",
    "MacBookPro-makmorit-jp:Wikipedia makmorit$ ls -al\n",
    "total 4868576\n",
    "drwxr-xr-x   4 makmorit  staff         136 May 22 13:44 .\n",
    "drwxr-xr-x  10 makmorit  staff         340 May 22 13:43 ..\n",
    "-rw-r--r--@  1 makmorit  staff       98379 Mar 23  2016 WikiExtractor.py\n",
    "-rw-r--r--@  1 makmorit  staff  2492605537 May 22 13:27 jawiki-latest-pages-articles.xml.bz2\n",
    "MacBookPro-makmorit-jp:Wikipedia makmorit$ python WikiExtractor.py -b 10M -o extracted jawiki-latest-pages-articles.xml.bz2\n",
    "INFO: Loaded 0 templates in 0.0s\n",
    "INFO: Starting page extraction from jawiki-latest-pages-articles.xml.bz2.\n",
    "INFO: Using 7 extract processes.\n",
    "WARNING: Template errors in article '自然数' (4671): title(9) recursion(0, 0, 0)\n",
    "WARNING: Template errors in article '階乗' (12763): title(1) recursion(0, 0, 0)\n",
    "INFO: Extracted 10000 articles (225.3 art/s)\n",
    "INFO: Extracted 20000 articles (263.3 art/s)\n",
    "（中略）\n",
    "WARNING: Template errors in article 'プロ経営者' (3516673): title(2) recursion(0, 0, 0)\n",
    "INFO: Extracted 1410000 articles (990.3 art/s)\n",
    "WARNING: Template errors in article '2017年のWTAツアー' (3535919): title(2) recursion(0, 0, 0)\n",
    "INFO: Extracted 1420000 articles (995.9 art/s)\n",
    "INFO: Extracted 1430000 articles (910.8 art/s)\n",
    "WARNING: Template errors in article 'Wikipedia:削除依頼/性液' (3583405): title(1) recursion(0, 0, 0)\n",
    "INFO: Extracted 1440000 articles (986.9 art/s)\n",
    "INFO: Finished 7-process extraction of 1446777 articles in 2059.6s (702.5 art/s)\n",
    "MacBookPro-makmorit-jp:Wikipedia makmorit$ date\n",
    "Mon May 22 14:25:52 JST 2017\n",
    "MacBookPro-makmorit-jp:Wikipedia makmorit$ \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## (2) 抽出コンテンツからTaggedDocumentを生成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### (2-1) 抽出コンテンツの内容についての注意\n",
    "\n",
    "下記のように、タイトルに : 文字が含まれている場合は有効な記事でないようです。\n",
    "\n",
    "```\n",
    "<doc id=\"1\" url=\"https://ja.wikipedia.org/wiki?curid=1\" title=\"Wikipedia:アップロードログ 2004年4月\">\n",
    "Wikipedia:アップロードログ 2004年4月\n",
    "<ul>\n",
    "</doc>\n",
    "```\n",
    "\n",
    "したがってTaggedDocumentへの変換時は、これらのエントリーは除外する必要があります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### (2-2) 抽出コンテンツをTaggedDocumentへ変換\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "有効なドキュメントは下記のようになっています。\n",
    "\n",
    "```\n",
    "<doc id=\"2466881\" url=\"https://ja.wikipedia.org/wiki?curid=2466881\" title=\"ヒントン駅\">\n",
    "ヒントン駅\n",
    "ヒントン駅（ヒントンえき、英語：Hinton Station）は、ウェストバージニア州 メープル・アベニューのセカンド・アヴェニュー100にある駅。 昔のチェサピーク・アンド・オハイオ鉄道の駅である。\n",
    "アムトラックの停車する列車は下記の通り。\n",
    "</doc>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "これを、以下のようにして、TaggedDocumentに変換します。\n",
    "\n",
    "分ち書きにはプロダクションのNlangクラスを使用します（すなわち品詞を一部落とします）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['ヒントン', '駅', 'ヒントン', '駅', 'ヒントン', 'える', '英語', 'Ｈｉｎｔｏｎ', 'Ｓｔａｔｉｏｎ', 'ウェストバージニア', '州', 'メープル・アベニュー', 'セカンド・アヴェニュー', '１００', '駅', '昔', 'チェサピーク・アンド・オハイオ', '鉄道', '駅', 'アムトラック', '停車', 'する', '列車', '下記', '通り'], tags=[2466881])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    環境準備\n",
    "'''\n",
    "import sys\n",
    "import os\n",
    "learning_dir = os.path.abspath(\"../../\") #<--- donusagi-bot/learning\n",
    "os.chdir(learning_dir)\n",
    "if learning_dir not in sys.path:\n",
    "    sys.path.append(learning_dir)\n",
    "\n",
    "'''\n",
    "    コンテンツファイルから逐一読み込み、\n",
    "    タグとコーパスをTaggedDocumentに設定\n",
    "'''\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from learning.core.nlang import Nlang\n",
    "\n",
    "\n",
    "# これは <doc> タグのidです\n",
    "tag = 2466881\n",
    "\n",
    "# これは <doc> タグの中身です\n",
    "sentences = '''ヒントン駅\n",
    "ヒントン駅（ヒントンえき、英語：Hinton Station）は、ウェストバージニア州 メープル・アベニューのセカンド・アヴェニュー100にある駅。 \n",
    "昔のチェサピーク・アンド・オハイオ鉄道の駅である。\n",
    "アムトラックの停車する列車は下記の通り。\n",
    "'''\n",
    "\n",
    "separated_sentences = Nlang.split(sentences)\n",
    "words = separated_sentences.split()\n",
    "\n",
    "tagged_document = TaggedDocument(words=words, tags=[tag])\n",
    "tagged_document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 全量の変換は未実施（上記処理を、抽出したWikipediaの全コンテンツについて実施する予定）。詳細は後報いたします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### (2-3) TaggedDocumentをインプットとして学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "今回はマイオペのテストデータは入れないでテストします。\n",
    "\n",
    "概ね下記のようなコードを実行して学習します。\n",
    "\n",
    "ただし分類数がわからないので、適当に500程度のサイズに設定しておきます。\n",
    "\n",
    "繰り返し回数はサイズの１０倍程度を指定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    "\n",
    "def doc2vec_model_path(dm):\n",
    "    model_path = 'prototype/better_algorithm/doc2vec.wikipedia.PV%d.model' % dm\n",
    "    return model_path\n",
    "\n",
    "def train(tagged_document_list, dm=0):\n",
    "    '''\n",
    "        tagged_document_listは、\n",
    "        すべてのWikipediaのタグ付きコーパス\n",
    "        (前述のtagged_document)が収容されたリスト。\n",
    "\n",
    "        これを引数にして、\n",
    "        ボキャブラリ生成／学習実行を実行します。\n",
    "        \n",
    "        dm=0でPV-DBOW、dm=1でPV-DMにより実行します。\n",
    "        （今回は比較のため、両方で試す予定）\n",
    "    '''\n",
    "    model = Doc2Vec(dm=dm, size=500, min_count=1, iter=5000)\n",
    "    model.build_vocab(tagged_document_list)\n",
    "    ret = model.train(tagged_document_list)\n",
    "\n",
    "    # 学習モデルは、ファイルに保存しておく\n",
    "    model.save(doc2vec_model_path(dm))\n",
    "    print('train_by_doc2vec: document vector size=%d, return=%d' % (len(model.docvecs), ret))\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    tagged_document_listを\n",
    "    使用し、学習実行\n",
    "'''\n",
    "train(tagged_document_list) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 結果は後報します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### (2-4) 適当な質問文で予測実行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "概ね下記のようなコードを実行して学習します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def predict(word, dm=0):\n",
    "    '''\n",
    "        予測処理にかけるコーパスを生成\n",
    "        （学習セット作成時と同じ関数を使用）\n",
    "    '''\n",
    "    corpus = Nlang.split(word).split()\n",
    "\n",
    "    '''\n",
    "        コーパスからベクトルを生成し、\n",
    "        ロードしたモデルから類似ベクトルを検索\n",
    "    '''\n",
    "    loaded_model = models.Doc2Vec.load(doc2vec_model_path(dm))\n",
    "    inferred_vector = loaded_model.infer_vector(corpus)\n",
    "    ret = loaded_model.docvecs.most_similar([inferred_vector])\n",
    "\n",
    "    return corpus, ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    契約書を見たいのですが\n",
    "'''\n",
    "predict('契約書を見たいのですが')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 結果は後報します。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
