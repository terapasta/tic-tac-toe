{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Doc2VecでWikipedia全文を使用\n",
    "\n",
    "Wiki全文の一部を、Doc2Vecのインプットに指定し、動作確認します。\n",
    "\n",
    "今回は２つの手法を比較しました。\n",
    "\n",
    "- PV-DBOW\n",
    "\n",
    "- PV-DM\n",
    "\n",
    "いずれも、質問文に対する予測結果が、学習セットと近似しているとは言い難い結果となりました。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## (1) Wikipediaコンテンツファイルから本文を抽出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a href=\"31-Wikipedia-contents-csv.ipynb\"><b>こちらの手順</b></a> にて、いったんローカルPCにCSVファイル化しておきます。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## (2) Wikipedia全文のTaggedDocumentを生成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Wikipedia全文のCSVファイルからデータフレームを読み込み、TaggedDocumentに変換します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    環境準備\n",
    "'''\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    " \n",
    "learning_dir = os.path.abspath(\"../../\") #<--- donusagi-bot/learning\n",
    "os.chdir(learning_dir)\n",
    "if learning_dir not in sys.path:\n",
    "    sys.path.append(learning_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Wikipedia全文から生成されたCSVを読み込み、\n",
    "    配列を生成する\n",
    "'''\n",
    "extracted_dir_path = '/Users/makmorit/Documents/Development/Wikipedia/extracted'\n",
    "csv_file_name = os.path.join(extracted_dir_path, 'Wikipedia-content.csv')\n",
    "df = pd.read_csv(csv_file_name)\n",
    "\n",
    "wiki_ids = np.array(df['id'])\n",
    "wiki_titles = np.array(df['title'])\n",
    "wiki_contents = np.array(df['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import MeCab\n",
    "import mojimoji\n",
    "\n",
    "class Nlang_naive:\n",
    "    @classmethod\n",
    "    def split(self, text):\n",
    "        tagger = MeCab.Tagger(\"-u learning/dict/custom.dic\")\n",
    "        tagger.parse('')  # node.surfaceを取得出来るようにするため、空文字をparseする(Python3のバグの模様)\n",
    "        node = tagger.parseToNode(text)\n",
    "        word_list = []\n",
    "        while node:\n",
    "            features = node.feature.split(\",\")\n",
    "            pos = features[0]\n",
    "            if pos in [\"BOS/EOS\", \"記号\"]:\n",
    "                node = node.next\n",
    "                continue\n",
    "\n",
    "            #print(features)\n",
    "            lemma = node.feature.split(\",\")[6]\n",
    "\n",
    "            if lemma == \"*\":\n",
    "                lemma = node.surface  #.decode(\"utf-8\")\n",
    "                \n",
    "            word_list.append(mojimoji.han_to_zen(lemma))\n",
    "            node = node.next\n",
    "        return \" \".join(word_list)\n",
    "\n",
    "    @classmethod\n",
    "    def batch_split(self, texts):\n",
    "        splited_texts = []\n",
    "        for text in texts:\n",
    "            splited_texts.append(self.split(text))\n",
    "        return splited_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import TaggedDocument\n",
    "# from learning.core.nlang import Nlang\n",
    "\n",
    "def get_tagged_document_list(wiki_ids, wiki_contents):\n",
    "    '''\n",
    "        タグとコーパスをTaggedDocumentに設定\n",
    "    '''\n",
    "    tagged_document_list = []\n",
    "\n",
    "    for index, _ in enumerate(wiki_ids):\n",
    "        tag = wiki_ids[index]\n",
    "        sentences = wiki_contents[index]\n",
    "\n",
    "        # 品詞は落とさない様にする\n",
    "        separated_sentences = Nlang_naive.split(sentences)\n",
    "        words = separated_sentences.split()\n",
    "        tagged_document_list.append(TaggedDocument(words=words, tags=[tag]))\n",
    "        \n",
    "    return tagged_document_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    時間がかかるので一部のみ使用します（全量＝100万件）\n",
    "'''\n",
    "partial_cnt = 1000\n",
    "\n",
    "wiki_ids = wiki_ids[0:partial_cnt]\n",
    "wiki_contents = wiki_contents[0:partial_cnt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wiki_tagged_document_list = get_tagged_document_list(wiki_ids, wiki_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成されたTaggedDocumentの一例をみてみます。\n",
    "\n",
    "品詞を落としていないせいか、かなり大きなコーパスであることが確認できます。\n",
    "\n",
    "（実は下記の例でも小さい方）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['アンパサンド', '（，', '＆）', 'と', 'は', 'と', 'を', '意味', 'する', '記号', 'だ', 'ある', '英語', 'の', 'に', '相当', 'する', 'ラテン語', 'の', 'の', '合', '字', 'で', '（', 'ｅｔ', 'ｃｅｔｅｒａ', '＝', 'ａｎｄ', 'ｓｏ', 'ｆｏｒｔｈ', '）', 'を', 'と', '記述', 'する', 'こと', 'が', 'ある', 'の', 'は', 'その', 'ため', 'Ｔｒｅｂｕｃｈｅｔ', 'ＭＳ', 'フォント', 'で', 'は', 'と', '表示', 'する', 'れる', '”', 'ｅｔ', '”', 'の', '合', '字', 'だ', 'ある', 'こと', 'が', '容易', 'に', 'わかる', 'その', '使用', 'は', '１', '世紀', 'に', '遡る', 'こと', 'が', 'できる', '（', '１', '）、', '５', '世紀', '中葉', '（', '２', '，', '３', '）', 'から', '現代', '（', '４', '−', '６', '）', 'に', '至る', 'まで', 'の', '変遷', 'が', 'わかる', 'Ｚ', 'に', '続く', 'ラテン', '文字', 'アルファベット', 'の', '２７', '字', '目', 'と', 'する', 'れる', 'た', '時期', 'も', 'ある', 'アンパサンド', 'と', '同じ', '役割', 'を', '果たす', '文字', 'に', 'の', 'ｅｔ', 'と', '呼ぶ', 'れる', '数字', 'の', '７', 'に', '似る', 'た', '記号', 'が', 'ある', 'た', '（，', 'Ｕ', '＋', '２０４', 'Ａ', '）。', 'この', '記号', 'は', '現在', 'も', 'ゲール', '文字', 'で', '使う', 'れる', 'て', 'いる', '記号', '名', 'の', 'アンパサンド', 'は', 'ラテン語', 'まじる', 'の', '英語', '「＆', 'は', 'それ', '自身', '”', 'ａｎｄ', '”', 'を', '表す', '（＆', 'ｐｅｒ', 'ｓｅ', 'ａｎｄ', '）', 'の', 'くずれる', 'た', '形', 'だ', 'ある', '英語', '以外', 'の', '言語', 'で', 'の', '名称', 'は', '多様', 'だ', 'ある', '日常', '的', 'だ', '手書き', 'の', '場合', '欧米', 'で', 'アンパサンド', 'は', 'に', '縦', '線', 'を', '引く', '単純', '化', 'する', 'れる', 'た', 'もの', 'が', '使う', 'れる', 'こと', 'が', 'ある', 'また', '同様', 'に', 'ｔ', 'または', '「＋（', 'プラス', 'に', '輪', 'を', '重ねる', 'た', 'よう', 'だ', '無声', '歯茎', '側面', '摩擦音', 'を', '示す', '発音', '記号', 'の', 'よう', 'だ', 'もの', 'が', '使う', 'れる', 'こと', 'も', 'ある', 'プログラミング', '言語', 'で', 'は', 'Ｃ', 'など', '多数', 'の', '言語', 'で', 'ＡＮＤ', '演算', '子', 'として', '用いる', 'られる', '以下', 'は', 'Ｃ', 'の', '例', 'ＰＨＰ', 'で', 'は', '変数', '宣言', '記号', '（＄）', 'の', '直前', 'に', '記述', 'する', 'こと', 'だ', '参照', '渡し', 'を', '行う', 'こと', 'が', 'できる', 'ＢＡＳＩＣ', '系列', 'の', '言語', 'で', 'は', '文字', '列', 'の', '連結', '演算', '子', 'として', '使用', 'する', 'れる', 'ｃｏｄｉｃｅ', '＿', '４', 'は', 'ｃｏｄｉｃｅ', '＿', '５', 'を', '返す', 'また', '主', 'に', 'マイクロソフト', '系', 'で', 'は', '整数', 'の', '十', '六', '進', '表記', 'に', 'ｃｏｄｉｃｅ', '＿', '６', 'を', '用いる', 'ｃｏｄｉｃｅ', '＿', '７', '十', '進', 'で', '１５', 'の', 'よう', 'に', '表現', 'する', 'ＳＧＭＬ', 'ＸＭＬ', 'ＨＴＭＬ', 'で', 'は', 'アンパサンド', 'を', '使う', 'て', 'ＳＧＭＬ', '実体', 'を', '参照', 'する'], tags=[5])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_tagged_document_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## (3) 学習実行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### (3-1) Wikipedia全文のTaggedDocumentだけをインプットとして学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "まずはマイオペのテストデータは入れないでテストします。\n",
    "\n",
    "概ね下記のようなコードを実行して学習します。\n",
    "\n",
    "分類数は不明なので、適当なサイズに設定しておきます（たとえばサンプルの10分の1とか）。\n",
    "\n",
    "繰り返し回数はサイズの10倍を指定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    "\n",
    "def doc2vec_model_path(dm):\n",
    "    model_path = 'prototype/better_algorithm/doc2vec.wikipedia.PV%d.model' % dm\n",
    "    return model_path\n",
    "\n",
    "def build_vocabulary(tagged_document_list, dm=0, size=100):\n",
    "    '''\n",
    "        tagged_document_listは、\n",
    "        すべてのWikipediaのタグ付きコーパス\n",
    "        (前述のtagged_document)が収容されたリスト。\n",
    "\n",
    "        これを引数にして、\n",
    "        ボキャブラリ生成を実行します。\n",
    "        \n",
    "        dm=0でPV-DBOW、dm=1でPV-DMにより実行します。\n",
    "    '''\n",
    "    model = Doc2Vec(dm=dm, size=size, min_count=1, iter=size*10)\n",
    "    model.build_vocab(tagged_document_list)\n",
    "\n",
    "    # 学習モデルは、ファイルに保存しておく\n",
    "    model.save(doc2vec_model_path(dm))\n",
    "    print('build_vocabulary: model saved temporary')\n",
    "\n",
    "    return model\n",
    "\n",
    "def train(model, tagged_document_list, dm=0):\n",
    "    '''\n",
    "        tagged_documentが収容されたリストを引数にして、\n",
    "        学習を実行します。\n",
    "        \n",
    "        dm=0でPV-DBOW、dm=1でPV-DMにより実行します。\n",
    "        （今回は比較のため、両方で試す）\n",
    "    '''\n",
    "    ret = model.train(tagged_document_list)\n",
    "\n",
    "    # 学習モデルは、ファイルに保存しておく\n",
    "    model.save(doc2vec_model_path(dm))\n",
    "    print('train: document vector size=%d, return=%d' % (len(model.docvecs), ret))\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build_vocabulary: model saved temporary\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    tagged_document_listを使用し、学習実行（PV-DBOW）\n",
    "'''\n",
    "model_dbow = build_vocabulary(wiki_tagged_document_list, dm=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: document vector size=1000, return=1193280113\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1193280113"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(model_dbow, wiki_tagged_document_list, dm=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build_vocabulary: model saved temporary\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    tagged_document_listを使用し、学習実行（PV-DM）\n",
    "'''\n",
    "model_dm = build_vocabulary(wiki_tagged_document_list, dm=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: document vector size=1000, return=1193261421\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1193261421"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(model_dm, wiki_tagged_document_list, dm=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## (3) 予測実行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "作成されたモデルを使用して予測を実行する関数になります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "\n",
    "def predict(word, dm=0):\n",
    "    '''\n",
    "        予測処理にかけるコーパスを生成\n",
    "        （学習セット作成時と同じ関数を使用）\n",
    "    '''\n",
    "    corpus = Nlang_naive.split(word).split()\n",
    "\n",
    "    '''\n",
    "        コーパスからベクトルを生成し、\n",
    "        ロードしたモデルから類似ベクトルを検索\n",
    "    '''\n",
    "    loaded_model = models.Doc2Vec.load(doc2vec_model_path(dm))\n",
    "    inferred_vector = loaded_model.infer_vector(corpus)\n",
    "    ret = loaded_model.docvecs.most_similar([inferred_vector])\n",
    "\n",
    "    return corpus, ret\n",
    "\n",
    "def get_title_from_tag(tag):\n",
    "    for index, _ in enumerate(wiki_ids):\n",
    "        if wiki_ids[index] == tag:\n",
    "            return wiki_titles[index]\n",
    "        \n",
    "    return None\n",
    "\n",
    "def print_predict_result(corpus, similarities):\n",
    "    print(corpus)\n",
    "\n",
    "    for tag, similarity in similarities:\n",
    "        print(get_title_from_tag(tag), similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### (3-1) 適当な質問文で予測（PV-DBOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['風邪', 'を', '引く', 'た', '時', 'の', '治療', '薬', 'を', '教える', 'て', 'くださる']\n",
      "ヘラクレイトス 0.4563015401363373\n",
      "ワンダフルライフ (映画) 0.40719297528266907\n",
      "安達哲 0.40467700362205505\n",
      "社会学者の一覧 0.4030371308326721\n",
      "築山 0.40055203437805176\n",
      "風邪 0.39788639545440674\n",
      "ゲーム 0.39261096715927124\n",
      "消滅した政権一覧 0.39236465096473694\n",
      "医療漫画 0.3832259178161621\n",
      "加瀬邦彦 0.3791019320487976\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    単語数がやや多い質問文を使い、質問をしてみます。\n",
    "'''\n",
    "corpus, ret = predict('風邪を引いた時の治療薬を教えてください', dm=0)\n",
    "print_predict_result(corpus, ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "元ねたと答え合わせ。\n",
    "\n",
    "元データには、２件ほど本文にキーワードがあるもの（id=329[風邪]と、id=655[水木しげる]）があります。\n",
    "\n",
    "ただし、予測結果（１件ヒット）としては類似度が低い感じです。\n",
    "```\n",
    "<doc id=\"329\" url=\"https://ja.wikipedia.org/wiki?curid=329\" title=\"風邪\">\n",
    "風邪（かぜ、common cold, nasopharyngitis, rhinopharyngitis, acute coryza, a cold）とは、ウイルスによる上気道感染症であり（以下略）\n",
    "\n",
    "<doc id=\"655\" url=\"https://ja.wikipedia.org/wiki?curid=655\" title=\"水木しげる\">\n",
    "（中略）帰還してまもなく行軍中に風邪を引いた際にマラリアを発症（以下略）\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['風邪']\n",
      "魔少年ビーティー 0.4811190962791443\n",
      "ウード 0.4510417580604553\n",
      "日本の漫画作品一覧 0.4497838616371155\n",
      "かくれんぼ 0.44944673776626587\n",
      "仮面ライダーV3 0.4454251527786255\n",
      "はしもとみつお 0.4416940212249756\n",
      "現代洋子 0.4305399954319\n",
      "出口竜正 0.42982280254364014\n",
      "大橋薫 0.42832446098327637\n",
      "くじらいいく子 0.42305099964141846\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    タイトルに近い文言で、Google検索っぽく質問をしてみます。\n",
    "'''\n",
    "corpus, ret = predict('風邪', dm=0)\n",
    "print_predict_result(corpus, ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "「風邪」が記述されているドキュメントに１件もヒットしませんでした。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### (3-2) 適当な質問文で予測（PV-DM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['風邪', 'を', '引く', 'た', '時', 'の', '治療', '薬', 'を', '教える', 'て', 'くださる']\n",
      "坂本タクマ 0.35875970125198364\n",
      "共通言語ランタイム 0.35062533617019653\n",
      "12月8日 0.35055220127105713\n",
      "風邪 0.33461111783981323\n",
      "みやすのんき 0.32742440700531006\n",
      "吉田里深 0.32369211316108704\n",
      "社会学者の一覧 0.3209966719150543\n",
      "イッセー尾形 0.31948035955429077\n",
      "元号から西暦への変換表 0.31912702322006226\n",
      "ゲームのタイトル一覧 0.3180711269378662\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    単語数がやや多い質問文を使い、質問をしてみます。\n",
    "'''\n",
    "corpus, ret = predict('風邪を引いた時の治療薬を教えてください', dm=1)\n",
    "print_predict_result(corpus, ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "「風邪」が記述されているドキュメントに１件ヒットしました（ただし類似度は低い）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['風邪']\n",
      "風邪 0.5769541263580322\n",
      "こしたてつひろ 0.5401366949081421\n",
      "野崎ふみこ 0.5135844945907593\n",
      "石井克人 0.5127018690109253\n",
      "幡地英明 0.5066384673118591\n",
      "是枝裕和 0.5041813254356384\n",
      "田島みるく 0.4961746633052826\n",
      "2003年 0.49040836095809937\n",
      "高瀬由香 0.48796403408050537\n",
      "のなかみのる 0.48767614364624023\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    タイトルに近い文言で、Google検索っぽく質問をしてみます。\n",
    "'''\n",
    "corpus, ret = predict('風邪', dm=1)\n",
    "print_predict_result(corpus, ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "「風邪」が記述されているドキュメントにかかってきました。\n",
    "\n",
    "PV-DMはこちらのような使い勝手にフィットしているのかもしれません。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
