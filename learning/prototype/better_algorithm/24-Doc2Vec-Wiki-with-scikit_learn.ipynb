{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Doc2vecの文書ベクトルを使用し、scikit-learnでコサイン類似検索\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- Wikipediaの文書からDoc2Vecモデルを生成\n",
    "\n",
    "\n",
    "- scikit-learnのcosine-simularityを使用して類似検索\n",
    "\n",
    "\n",
    "昨日検証時のモデルファイルをそのまま使用しています。\n",
    "\n",
    "nosetestsの質問文を使用してテストしたところ、４問中１問だけが正解、という結果に終わりました。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## (1) Wikipediaコンテンツファイルから全文書を抽出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a href=\"31-Wikipedia-contents-csv.ipynb\"><b>こちらの手順</b></a> にて、いったんローカルPCにCSVファイル化しておきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## (2) Wikipedia文書を学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a href=\"23-Doc2Vec-with-Wiki.ipynb\"><b>こちらの手順</b></a> にて生成したDoc2Vecモデルファイルをロードして使用します。\n",
    "\n",
    "上記手順では、Wikipedia文書のみを使用し、ボキャブラリ／単語ベクトルの生成および学習を行い、モデルをファイル保存しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    環境準備\n",
    "'''\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    " \n",
    "learning_dir = os.path.abspath(\"../../\") #<--- donusagi-bot/learning\n",
    "os.chdir(learning_dir)\n",
    "if learning_dir not in sys.path:\n",
    "    sys.path.append(learning_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "\n",
    "def doc2vec_model_path(dm):\n",
    "    model_path = 'prototype/better_algorithm/doc2vec.wikipedia.PV%d.model' % dm\n",
    "    return model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document vector size=87181\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    あらかじめ学習したモデルのファイルをロード\n",
    "    dm = 0 : DBoWを使用したモデル\n",
    "'''\n",
    "dm = 0\n",
    "loaded_model_dbow = models.Doc2Vec.load(doc2vec_model_path(dm))\n",
    "\n",
    "print('Document vector size=%d' % (len(loaded_model_dbow.docvecs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## (3) my-opeの文書を、Wikiから生成したモデルにより文書ベクトル化する関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Wikipedia文書だけで学習されたDoc2Vecモデルを使用し、my-ope文書（質問文）をベクトル化します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from learning.core.learn.learning_parameter import LearningParameter\n",
    "from learning.core.datasource import Datasource\n",
    "\n",
    "_bot_id = 13 # toyotsu_human.csv\n",
    "attr = {\n",
    "    'include_failed_data': False,\n",
    "    'include_tag_vector': False,\n",
    "    'classify_threshold': 0.5,\n",
    "    'algorithm': LearningParameter.ALGORITHM_LOGISTIC_REGRESSION,\n",
    "    'params_for_algorithm': {'C': 140},\n",
    "    'excluded_labels_for_fitting': None\n",
    "}\n",
    "learning_parameter = LearningParameter(attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import MeCab\n",
    "import mojimoji\n",
    "\n",
    "class Nlang_naive:\n",
    "    @classmethod\n",
    "    def split(self, text):\n",
    "        tagger = MeCab.Tagger(\"-u learning/dict/custom.dic\")\n",
    "        tagger.parse('')  # node.surfaceを取得出来るようにするため、空文字をparseする(Python3のバグの模様)\n",
    "        node = tagger.parseToNode(text)\n",
    "        word_list = []\n",
    "        while node:\n",
    "            features = node.feature.split(\",\")\n",
    "            pos = features[0]\n",
    "            if pos in [\"BOS/EOS\", \"記号\"]:\n",
    "                node = node.next\n",
    "                continue\n",
    "\n",
    "            #print(features)\n",
    "            lemma = node.feature.split(\",\")[6]\n",
    "\n",
    "            if lemma == \"*\":\n",
    "                lemma = node.surface  #.decode(\"utf-8\")\n",
    "                \n",
    "            word_list.append(mojimoji.han_to_zen(lemma))\n",
    "            node = node.next\n",
    "        return \" \".join(word_list)\n",
    "\n",
    "    @classmethod\n",
    "    def batch_split(self, texts):\n",
    "        splited_texts = []\n",
    "        for text in texts:\n",
    "            splited_texts.append(self.split(text))\n",
    "        return splited_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_document_vector(question, model):\n",
    "    '''\n",
    "        question: \n",
    "            分かち書きされていない文書\n",
    "        model:\n",
    "            Doc2Vecの学習済みモデル\n",
    "            （検証時は品詞を落としていないWikipedia文書からモデルを生成）\n",
    "\n",
    "        inferred_vector:\n",
    "            文書を分かち書きしたコーパスから、\n",
    "            Doc2Vecの学習済みモデルを使用して\n",
    "            生成される類似文書ベクトル\n",
    "            （モデルに合わせ、品詞は落とさない様にする）\n",
    "    '''\n",
    "    corpus = Nlang_naive.split(question).split()\n",
    "    inferred_vector = model.infer_vector(corpus)\n",
    "\n",
    "    return inferred_vector\n",
    "\n",
    "def get_document_vectors(questions, model):\n",
    "    document_vectors = []\n",
    "    for question in questions:\n",
    "        inferred_vector = get_document_vector(question, model)\n",
    "        document_vectors.append(list(inferred_vector))\n",
    "\n",
    "    return np.array(document_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## (4) コサイン類似検索の実行\n",
    "\n",
    "質問文は、my-ope プロダクションの nosetests テストケースから引用しました。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from learning.core.datasource import Datasource\n",
    "\n",
    "def search_simiarity(question, dbow_model):\n",
    "    '''\n",
    "        質問文間でコサイン類似度を算出して、近い質問文の候補を取得する\n",
    "        \n",
    "        仕様はプロダクションに準拠しています\n",
    "        ただし、文書のベクトル化は、TF-IDFではなく、\n",
    "        Doc2Vecを使用します。\n",
    "    '''\n",
    "    datasource = Datasource('csv')\n",
    "    question_answers = datasource.question_answers_for_suggest(_bot_id, question)\n",
    "\n",
    "    #all_array = TextArray(question_answers['question'], vectorizer=self.vectorizer)\n",
    "    #question_array = TextArray([question], vectorizer=self.vectorizer)\n",
    "    all_array      = get_document_vectors(question_answers['question'], dbow_model)\n",
    "    question_array = get_document_vectors([question], dbow_model)\n",
    "    \n",
    "    print('count: my-ope all questions=%d, document vectors=%d (features=%d)' % (\n",
    "        np.size(question_answers['question']), all_array.shape[0], all_array.shape[1]\n",
    "    ))    \n",
    "    print('count: question=%d, document vectors=%d (features=%d)' % (\n",
    "        np.size([question]), question_array.shape[0], question_array.shape[1]\n",
    "    ))    \n",
    "\n",
    "    similarities = cosine_similarity(all_array, question_array)\n",
    "    similarities = similarities.flatten()\n",
    "\n",
    "    ordered_result = list(map(lambda x: {\n",
    "        'question_answer_id': float(x[0]), 'similarity': x[1], 'answer_id': x[2]\n",
    "    }, sorted(zip(question_answers['id'], similarities, question_answers['answer_id']), key=lambda x: x[1], reverse=True)))\n",
    "\n",
    "    df = pd.DataFrame.from_dict(ordered_result)\n",
    "\n",
    "    print(df[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017/05/29 PM 03:44:01 ['./fixtures/learning_training_messages/benefitone.csv', './fixtures/learning_training_messages/ptna.csv', './fixtures/learning_training_messages/septeni.csv', './fixtures/learning_training_messages/toyotsu_human.csv']\n",
      "2017/05/29 PM 03:44:01 ['./fixtures/question_answers/toyotsu_human.csv']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count: my-ope all questions=317, document vectors=317 (features=200)\n",
      "count: question=1, document vectors=1 (features=200)\n",
      "   answer_id  question_answer_id  similarity\n",
      "0       6819             13393.0    0.195777\n",
      "1       6823             13397.0    0.152361\n",
      "2       6802             13377.0    0.141884\n",
      "3       6911             13485.0    0.140777\n",
      "4       6745             13317.0    0.135494\n",
      "5       6860             13434.0    0.127516\n",
      "6       6968             13548.0    0.126695\n",
      "7       6986             13566.0    0.125811\n",
      "8       6834             13408.0    0.120590\n",
      "9       6966             13546.0    0.115356\n"
     ]
    }
   ],
   "source": [
    "# 正解＝6803\n",
    "search_simiarity('JAL マイレージ', loaded_model_dbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017/05/29 PM 03:44:02 ['./fixtures/learning_training_messages/benefitone.csv', './fixtures/learning_training_messages/ptna.csv', './fixtures/learning_training_messages/septeni.csv', './fixtures/learning_training_messages/toyotsu_human.csv']\n",
      "2017/05/29 PM 03:44:02 ['./fixtures/question_answers/toyotsu_human.csv']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count: my-ope all questions=317, document vectors=317 (features=200)\n",
      "count: question=1, document vectors=1 (features=200)\n",
      "   answer_id  question_answer_id  similarity\n",
      "0       6749             13310.0    0.506196\n",
      "1       6759             13331.0    0.479292\n",
      "2       6766             13338.0    0.464680\n",
      "3       6743             13314.0    0.441656\n",
      "4       6856             13430.0    0.432572\n",
      "5       6744             13316.0    0.414209\n",
      "6       6863             13437.0    0.411949\n",
      "7       6763             13335.0    0.410406\n",
      "8       6851             13425.0    0.409212\n",
      "9       6792             13367.0    0.398344\n"
     ]
    }
   ],
   "source": [
    "# 正解＝6763\n",
    "search_simiarity('海外の出張費の精算の方法は？', loaded_model_dbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017/05/29 PM 03:44:02 ['./fixtures/learning_training_messages/benefitone.csv', './fixtures/learning_training_messages/ptna.csv', './fixtures/learning_training_messages/septeni.csv', './fixtures/learning_training_messages/toyotsu_human.csv']\n",
      "2017/05/29 PM 03:44:02 ['./fixtures/question_answers/toyotsu_human.csv']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count: my-ope all questions=317, document vectors=317 (features=200)\n",
      "count: question=1, document vectors=1 (features=200)\n",
      "   answer_id  question_answer_id  similarity\n",
      "0       6710             13280.0    0.350886\n",
      "1       6916             13495.0    0.350341\n",
      "2       6775             13348.0    0.327487\n",
      "3       6933             13512.0    0.327060\n",
      "4       6917             13496.0    0.325855\n",
      "5       6807             13382.0    0.323945\n",
      "6       6859             13433.0    0.323725\n",
      "7       6787             13360.0    0.322733\n",
      "8       6907             13481.0    0.322318\n",
      "9       6804             13379.0    0.321103\n"
     ]
    }
   ],
   "source": [
    "# 正解＝6767\n",
    "search_simiarity('VISAの勘定科目がわからない', loaded_model_dbow) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017/05/29 PM 03:44:02 ['./fixtures/learning_training_messages/benefitone.csv', './fixtures/learning_training_messages/ptna.csv', './fixtures/learning_training_messages/septeni.csv', './fixtures/learning_training_messages/toyotsu_human.csv']\n",
      "2017/05/29 PM 03:44:03 ['./fixtures/question_answers/toyotsu_human.csv']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count: my-ope all questions=317, document vectors=317 (features=200)\n",
      "count: question=1, document vectors=1 (features=200)\n",
      "   answer_id  question_answer_id  similarity\n",
      "0       6909             13483.0    0.413178\n",
      "1       6985             13565.0    0.287570\n",
      "2       6871             13445.0    0.282850\n",
      "3       6853             13427.0    0.274783\n",
      "4       6773             13345.0    0.271242\n",
      "5       6807             13382.0    0.270986\n",
      "6       6924             13503.0    0.269384\n",
      "7       6986             13566.0    0.268611\n",
      "8       6738             13324.0    0.260625\n",
      "9       6888             13462.0    0.260544\n"
     ]
    }
   ],
   "source": [
    "# 正解＝6909\n",
    "search_simiarity('子供が生まれた', loaded_model_dbow) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
