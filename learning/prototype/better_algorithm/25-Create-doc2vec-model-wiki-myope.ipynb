{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Wikipedia＋my-ope文書を使用しDoc2Vecモデルを生成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## (1) Wikipediaコンテンツファイルから全文書を抽出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "レポート <a href=\"31-Wikipedia-contents-csv.ipynb\"><b>31-Wikipedia-contents-csv.ipynb</b></a> の手順にて、いったんローカルPCにCSVファイル化しておきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## (2) Wikipedia文書のTaggedDocumentを生成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Wikipedia文書のCSVファイルからデータフレームを読み込み、TaggedDocumentに変換します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    環境準備\n",
    "'''\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    " \n",
    "learning_dir = os.path.abspath(\"../../\") #<--- donusagi-bot/learning\n",
    "os.chdir(learning_dir)\n",
    "if learning_dir not in sys.path:\n",
    "    sys.path.append(learning_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import MeCab\n",
    "import mojimoji\n",
    "\n",
    "class Nlang_naive:\n",
    "    @classmethod\n",
    "    def split(self, text):\n",
    "        tagger = MeCab.Tagger(\"-u learning/dict/custom.dic\")\n",
    "        tagger.parse('')  # node.surfaceを取得出来るようにするため、空文字をparseする(Python3のバグの模様)\n",
    "        node = tagger.parseToNode(text)\n",
    "        word_list = []\n",
    "        while node:\n",
    "            features = node.feature.split(\",\")\n",
    "            pos = features[0]\n",
    "            if pos in [\"BOS/EOS\", \"記号\"]:\n",
    "                node = node.next\n",
    "                continue\n",
    "\n",
    "            #print(features)\n",
    "            lemma = node.feature.split(\",\")[6]\n",
    "\n",
    "            if lemma == \"*\":\n",
    "                lemma = node.surface  #.decode(\"utf-8\")\n",
    "                \n",
    "            word_list.append(mojimoji.han_to_zen(lemma))\n",
    "            node = node.next\n",
    "        return \" \".join(word_list)\n",
    "\n",
    "    @classmethod\n",
    "    def batch_split(self, texts):\n",
    "        splited_texts = []\n",
    "        for text in texts:\n",
    "            splited_texts.append(self.split(text))\n",
    "        return splited_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import TaggedDocument\n",
    "#from learning.core.nlang import Nlang\n",
    "\n",
    "def get_tagged_document_list(wiki_ids, wiki_contents):\n",
    "    '''\n",
    "        タグとコーパスをTaggedDocumentに設定\n",
    "    '''\n",
    "    tagged_document_list = []\n",
    "\n",
    "    for index, _ in enumerate(wiki_ids):\n",
    "        tag = wiki_ids[index]\n",
    "\n",
    "        sentences = wiki_contents[index]\n",
    "        separated_sentences = Nlang_naive.split(sentences)\n",
    "        words = separated_sentences.split()\n",
    "\n",
    "        tagged_document_list.append(TaggedDocument(words=words, tags=[tag]))\n",
    "        \n",
    "    return tagged_document_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Wikipedia文書から生成されたCSVを読み込み、\n",
    "    配列を生成する\n",
    "'''\n",
    "extracted_dir_path = '/Users/makmorit/Documents/Development/Wikipedia/extracted'\n",
    "csv_file_name = os.path.join(extracted_dir_path, 'Wikipedia-content.csv')\n",
    "df = pd.read_csv(csv_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "_wiki_ids = np.array(df['id'])\n",
    "_wiki_titles = np.array(df['title'])\n",
    "_wiki_contents = np.array(df['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    時間がかかるので一部のみ使用します（全量＝100万件）\n",
    "'''\n",
    "partial_cnt = 10000\n",
    "\n",
    "def split_wiki_corpus(wiki_id, wiki_content, wiki_title):\n",
    "    '''\n",
    "        コーパスを分割\n",
    "\n",
    "        予測処理のインプットとなる単語数に合わせるため、\n",
    "        句読点でコーパスを分割する。\n",
    "        ただし、この段階でIDをユニーク化すると\n",
    "        分割されたコーパス間で意味が繋がらなくなるため、\n",
    "        IDを [doc id]_[serial] 形式で付番する\n",
    "    '''\n",
    "    sub_ids = []\n",
    "    sub_contents = []\n",
    "    titles = []\n",
    "    \n",
    "    sub_index = 0\n",
    "    for sub_content in wiki_content.split('。'):\n",
    "        if len(sub_content) == 0:\n",
    "            continue\n",
    "        if sub_content[0] == ' ':\n",
    "            continue\n",
    "\n",
    "        sub_id = '%d_%d' % (wiki_id, sub_index)\n",
    "        sub_ids.append(sub_id)\n",
    "        sub_contents.append(sub_content)\n",
    "        titles.append(wiki_title)\n",
    "        sub_index += 1\n",
    "        \n",
    "        if sub_index == 10:\n",
    "            break\n",
    "\n",
    "    return sub_ids, sub_contents, titles\n",
    "\n",
    "wiki_ids = []\n",
    "wiki_contents = []\n",
    "wiki_titles = []\n",
    "\n",
    "for index in range(partial_cnt):\n",
    "    wiki_id = _wiki_ids[index]\n",
    "    wiki_content = _wiki_contents[index]\n",
    "    wiki_title = _wiki_titles[index]\n",
    "\n",
    "    sub_ids, sub_contents, titles = split_wiki_corpus(wiki_id, wiki_content, wiki_title)\n",
    "\n",
    "    wiki_ids.extend(sub_ids)\n",
    "    wiki_contents.extend(sub_contents)\n",
    "    wiki_titles.extend(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wiki_tagged_document_list = get_tagged_document_list(wiki_ids, wiki_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87031"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wiki_tagged_document_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87031"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wiki_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## (3) my-ope文書のTaggedDocumentを生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from learning.core.learn.learning_parameter import LearningParameter\n",
    "from learning.core.datasource import Datasource\n",
    "\n",
    "_bot_id = 13 # toyotsu_human.csv\n",
    "attr = {\n",
    "    'include_failed_data': False,\n",
    "    'include_tag_vector': False,\n",
    "    'classify_threshold': 0.5,\n",
    "    'algorithm': LearningParameter.ALGORITHM_LOGISTIC_REGRESSION,\n",
    "    'params_for_algorithm': {'C': 140},\n",
    "    'excluded_labels_for_fitting': None\n",
    "}\n",
    "learning_parameter = LearningParameter(attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017/06/01 AM 11:08:51 ['./fixtures/learning_training_messages/benefitone.csv', './fixtures/learning_training_messages/ptna.csv', './fixtures/learning_training_messages/septeni.csv', './fixtures/learning_training_messages/toyotsu_human.csv']\n",
      "2017/06/01 AM 11:08:51 ['./fixtures/question_answers/toyotsu_human.csv']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    マイオペの学習セットを取得\n",
    "'''\n",
    "_datasource = Datasource(type='csv')\n",
    "#learning_training_messages = _datasource.learning_training_messages(_bot_id)\n",
    "learning_training_messages = _datasource.question_answers(_bot_id)\n",
    "\n",
    "questions = np.array(learning_training_messages['question'])\n",
    "answer_ids = np.array(learning_training_messages['answer_id'])\n",
    "ids = np.array(learning_training_messages['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "317"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    既存セットの否定の意味を持つトレーニングセットを追加します。\n",
    "    例）\n",
    "    　'「退職金」退職金の振込口座を変えたい（正解＝6975）'\n",
    "    　に対して、否定の意味を持つセット\n",
    "    　'「退職金」退職金の振込口座を変えたくない'\n",
    "'''\n",
    "questions_nega = np.array(['「退職金」退職金の振込口座を変えたくない'])\n",
    "questions = np.append(questions, questions_nega)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "answer_ids_nega = np.array([7050])\n",
    "answer_ids = np.append(answer_ids, answer_ids_nega)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ids_nega = np.array([13700])\n",
    "ids = np.append(ids, ids_nega)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count: questions=318, answer_ids=318 (class count=303), ids=318\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    正しく要素が追加されたかどうかチェックします。\n",
    "'''\n",
    "print('count: questions=%d, answer_ids=%d (class count=%d), ids=%d' % (\n",
    "    np.size(questions), np.size(answer_ids), np.size(np.unique(answer_ids)), np.size(ids)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "318"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    マイオペの学習セットから、コーパスを生成\n",
    "    \n",
    "    タグとして、learning_training_messages.id を付与\n",
    "'''\n",
    "myope_tagged_document_list = get_tagged_document_list(ids, questions)\n",
    "len(myope_tagged_document_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## (4) 両方のTaggedDocumentを合体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "all_tagged_document_list = []\n",
    "all_tagged_document_list.extend(wiki_tagged_document_list)\n",
    "all_tagged_document_list.extend(myope_tagged_document_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87349"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_tagged_document_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## (5) 学習実行とモデルファイル作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    "import time\n",
    "\n",
    "def doc2vec_model_path(dm):\n",
    "    model_path = 'prototype/better_algorithm/doc2vec.wiki_myope.PV%d.model' % dm\n",
    "    return model_path\n",
    "\n",
    "def train(tagged_document_list, dm, size):\n",
    "    '''\n",
    "        tagged_document_listは、\n",
    "        Wikipedia文書のタグ付きコーパス\n",
    "        (前述のtagged_document)が収容されたリスト。\n",
    "\n",
    "        これを引数にして、\n",
    "        ボキャブラリ生成と学習を実行します。\n",
    "        \n",
    "        dm=0でPV-DBOW、dm=1でPV-DMにより実行します。\n",
    "        学習が完了したら、モデルをファイルに保存します。\n",
    "    '''\n",
    "    start = time.time()\n",
    "    model = Doc2Vec(documents=tagged_document_list, dm=dm, size=size, \n",
    "                    window=20, min_count=1, iter=size*10, workers=100)\n",
    "    elapsed_time =  time.time() - start\n",
    "    if dm:\n",
    "        s = 'DM'\n",
    "    else:\n",
    "        s = 'DBoW'\n",
    "    print(\"Doc2Vec: trained by %s: elapsed %d seconds\" % (s, elapsed_time))\n",
    "    print('Doc2Vec: document vector size=%d, feature size=%d' % (len(model.docvecs), size))\n",
    "\n",
    "    model.save(doc2vec_model_path(dm))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec: trained by DBoW: elapsed 6317 seconds\n",
      "Doc2Vec: document vector size=87349, feature size=200\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    tagged_document_listを使用し、学習実行（PV-DBOW）\n",
    "'''\n",
    "model_dbow = train(all_tagged_document_list, dm=0, size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#'''\n",
    "#    tagged_document_listを使用し、学習実行（PV-DM）\n",
    "#'''\n",
    "#model_dm = train(all_tagged_document_list, dm=1, size=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
