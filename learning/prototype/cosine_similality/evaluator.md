情報検索エンジンの評価指標としてrecall,precision,F-scoreがあるが、
コサイン類似検索にも適用できるかどうかを調査。

## precision
適合率(precision, 精度)とは、`システムが出した結果`において、`本当に正しかったもの`の割合。検索対象の文書群の中から、正しく検索された文書の割合を指す。

## recall
再現率(recall)とは、結果として出てくるべきもの(記事や文書)のうち、実際に出て来たものの割合。網羅性に関する指標。

## F-score
正確性と網羅性の総合的な評価の際に利用される尺度のこと
適合率、再現率のバランスを数値化したようなイメージ


# コサイン類似検索に上記を適用するするには?
- 本当に正しかったかどうかをデータとして持たせることで可能
  - 本当にただしいかどうかは人の手で入力するしかなさそう


## 実装案１

精度を判定するときに以下を実施する

1. messagesテーブルに入っているbodyを取得し、replyを取得する。-> `システムが出した結果`
2. messagesテーブルに入っているbodyを取得し、正しい結果の一覧を人力で作成する -> `本当に正しかったもの`
3. messagesテーブルに入っているbodyを取得し、少しでも一致する結果の一覧を人力で作成する -> `結果として出てくるべきもの`
4. 適合率、再現率、F値を出す

### ポイント
- 本当に正しかったものは人力で入力するので、指標として信頼できる
- 人力で入力するので手間がかかる

## 実装案２

1. similarityが0より大きいものを`結果`とする
2. similarityのしきい値を超えたもの`本当に正しかったもの`とする
3. 質問に含まれている単語のいずれかを含むquestion_answersを`結果として出てくるべきもの`とする
4. 適合率、再現率、F値を出す

### ポイント
- 本当に正しかったものを自動的に出しているため、指標として危うい

# 検討結果
実装案1でやりましょう

# その他

- 平均精度という指標もある
- いろいろな評価指標があって面白い
  - http://blog.brainpad.co.jp/entry/2017/08/25/140000
- 正解データをどう集めるのか 
  - http://www.atmarkit.co.jp/ait/articles/1507/29/news010_2.html#05
> しかし、何か重要なことを見逃していないでしょうか。これまでに、以下の二つの問題点を十分に考慮せずにいたのです。

> そもそも「カスタマーの本来求めていたドキュメント」とは何か？
> 検索順位（検索ランキング）は問題にならないのか？
> 　「カスタマーの本来求めていたドキュメント」の説明の際、それはあくまで理想の検索結果と説明しました。一般的に、全ての「カスタマーの本来求めていたドキュメント」を収集して、検索結果と突き合わせ、適合率・再現率を求めることは非現実的です。
> 　現実的な「カスタマーの本来求めていたドキュメント」は、用意可能な量のサンプルになり、「機械学習」の枠組みで考えれば、あらかじめ用意された正解データであり、教師あり学習における訓練データ、教師データです。
> 　多くの場合、次の2つが「カスタマーの本来求めていたドキュメント」のソースとなるでしょう。
> 人為的に作成した正解データ：主観的に作成した「クエリ」と「カスタマーの本来求めていたドキュメント」の対
> 検索ログ：検索システムを利用したカスタマーが検索を行った際の「クエリ」と「アクションのログ」の対
> 　それでは、これらのデータを用いて検索の精度を測るにはどのようにしたらよいでしょうか？ そして、もう一つ考慮せずにいた検索ランキングの問題は解決できるでしょうか？ ここで、大きく問題の転換を図ります。
