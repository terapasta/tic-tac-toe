{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import MeCab\n",
    "import mojimoji\n",
    "\n",
    "\n",
    "class MecabTokenizer:\n",
    "    def __init__(self):\n",
    "        self.tagger = MeCab.Tagger(\"-u dict/custom.dic\")\n",
    "        # Note: node.surfaceを取得出来るようにするため、空文字をparseする(Python3のバグの模様)\n",
    "        self.tagger.parse('')\n",
    "\n",
    "    def tokenize(self, texts):\n",
    "        splited_texts = []\n",
    "        for text in texts:\n",
    "            splited_texts.append(self.tokenize_single_text(text))\n",
    "        return splited_texts\n",
    "\n",
    "    def tokenize_single_text(self, text):\n",
    "        node = self.tagger.parseToNode(text)\n",
    "        word_list = []\n",
    "        while node:\n",
    "            features = node.feature.split(',')\n",
    "            pos = features[0]\n",
    "\n",
    "            if pos in [\"名詞\", \"動詞\", \"形容詞\", \"感動詞\", \"助動詞\", \"副詞\"]:\n",
    "                lemma = node.feature.split(\",\")[6]\n",
    "\n",
    "                if pos == '名詞' and features[1] == '非自立':\n",
    "                    node = node.next\n",
    "                    continue\n",
    "                if pos == '動詞' and features[1] == '非自立':\n",
    "                    node = node.next\n",
    "                    continue\n",
    "\n",
    "                if pos == '助動詞' and lemma != 'ない':\n",
    "                    node = node.next\n",
    "                    continue\n",
    "\n",
    "                if lemma == 'ある':\n",
    "                    node = node.next\n",
    "                    continue\n",
    "\n",
    "                if lemma == \"*\":\n",
    "                    lemma = node.surface\n",
    "\n",
    "                word_list.append(mojimoji.han_to_zen(lemma))\n",
    "            node = node.next\n",
    "        return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_table('/Users/shwld/Downloads/wiki-corpus/jawiki.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SystemError",
     "evalue": "<built-in function Tagger_parseToNode> returned a result with an error set",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-3bec9e040740>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenized_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMecabTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-82e852f7ff5e>\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0msplited_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0msplited_texts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize_single_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msplited_texts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-82e852f7ff5e>\u001b[0m in \u001b[0;36mtokenize_single_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtokenize_single_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mnode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparseToNode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mword_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shwld/.pyenv/versions/3.5.2/envs/donusagi-bot3/lib/python3.5/site-packages/MeCab.py\u001b[0m in \u001b[0;36mparseToNode\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0m__repr__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_swig_repr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0m_MeCab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTagger_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mparseToNode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0m_MeCab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTagger_parseToNode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparseNBest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0m_MeCab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTagger_parseNBest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparseNBestInit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0m_MeCab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTagger_parseNBestInit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSystemError\u001b[0m: <built-in function Tagger_parseToNode> returned a result with an error set"
     ]
    }
   ],
   "source": [
    "tokenized_sentences = MecabTokenizer().tokenize(data['question'])\n",
    "joblib.dump(tokenized_sentences, '/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tokenized_sentences = joblib.load('/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tokenized_sentences = [\n",
    "#     ['アンパサンド', '（，', '＆）', '意味', 'する', '記号', 'ラテン語', '合', '字', 'Ｔｒｅｂｕｃｈｅｔ', 'ＭＳ', 'フォント', '表示', 'する', 'れる', '”', 'ｅｔ', '”', '合', '字', '容易', 'わかる', 'ａｍｐｅｒｓａ', '”', 'ａｎｄ', 'ｐｅｒ', 'ｓｅ', 'ａｎｄ', '”、', '意味', '”', 'ａｎｄ', '［', 'ｔｈｅ', 'ｓｙｍｂｏｌ', 'ｗｈｉｃｈ', '］', 'ｂｙ', 'ｉｔｓｅｌｆ', '［', 'ｉｓ', '］', 'ａｎｄ', '”'],\n",
    "#     ['使用', '１', '世紀', '遡る', 'できる', '（', '１', '）、', '５', '世紀', '中葉', '（', '２', '，', '３', '）', '現代', '（', '４', '−', '６', '）', '至る', '変遷', 'わかる'],\n",
    "#     ['Ｚ', '続く', 'ラテン', '文字', 'アルファベット', '２７', '字', '目', 'する', 'れる', '時期'],\n",
    "#     ['アンパサンド', '役割', '果たす', '文字', 'ｅｔ', '呼ぶ', 'れる', '数字', '７', '似る', '記号', '（，', 'Ｕ', '＋', '２０４', 'Ａ', '）。', '記号', '現在', 'ゲール', '文字', '使う', 'れる'],\n",
    "#     ['記号', '名', 'アンパサンド', 'ラテン語', 'まじる', '英語', '「＆', 'それ', '自身', '”', 'ａｎｄ', '”', '表す', '（＆', 'ｐｅｒ', 'ｓｅ', 'ａｎｄ', '）', 'くずれる', '形', '英語', '言語', '名称', '多様'],\n",
    "#     ['日常', '的', '手書き', '場合', '欧米', 'アンパサンド', '縦', '線', '引く', '単純', '化', 'する', 'れる', '使う', 'れる'],\n",
    "#     ['同様', 'ｔ', '「＋（', 'プラス', '輪', '重ねる', '無声', '歯茎', '側面', '摩擦音', '示す', '発音', '記号', '使う', 'れる'],\n",
    "#     ['プログラミング', '言語', 'Ｃ', '多数', '言語', 'ＡＮＤ', '演算', '子', '用いる', 'られる', 'Ｃ', '例'],\n",
    "#     ['ＰＨＰ', '変数', '宣言', '記号', '（＄）', '直前', '記述', 'する', '参照', '渡し', '行う', 'できる'],\n",
    "#     ['ＢＡＳＩＣ', '系列', '言語', '文字', '列', '連結', '演算', '子', '使用', 'する', 'れる', 'ｃｏｄｉｃｅ', '＿', '４', 'ｃｏｄｉｃｅ', '＿', '５', '返す', '主', 'マイクロソフト', '系', '整数', '十', '六', '進', '表記', 'ｃｏｄｉｃｅ', '＿', '６', '用いる', 'ｃｏｄｉｃｅ', '＿', '７', '十', '進', '１５', '表現', 'する']\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gensim_dictionary=corpora.Dictionary(tokenized_sentences)\n",
    "gensim_dictionary.save('/Users/shwld/Downloads/wiki-corpus/gensim.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#corpusをid表示に変換する．二次元リストで構築しているので内包表記で記述\n",
    "id_corpus=[gensim_dictionary.doc2bow(sentence) for sentence in tokenized_sentences];\n",
    "#保存しておく \n",
    "corpora.MmCorpus.serialize('/Users/shwld/Downloads/wiki-corpus/gensim.mm', id_corpus);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tfidfモデルの構築とcorpusの変換\n",
    "tfidf=models.TfidfModel(id_corpus)\n",
    "tfidf.save('/Users/shwld/Downloads/wiki-corpus/gensim.tfidf')\n",
    "tfidf_corpus=tfidf[id_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#lsiモデルの構築とcorpusの変換\n",
    "lsi=models.LsiModel(corpus=tfidf_corpus, num_topics=10, id2word=gensim_dictionary)\n",
    "lsi.save('/Users/shwld/Downloads/wiki-corpus/gensim.lsi')\n",
    "lsi_corpus=lsi[tfidf_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lsi_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "queries = ['マイクロソフト', '演算']\n",
    "query_vector = gensim_dictionary.doc2bow(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.03736972832018761),\n",
       " (1, 0.14905692857376651),\n",
       " (2, -0.21773407396425185),\n",
       " (3, -0.097940788364416956),\n",
       " (4, 0.027534233926046335),\n",
       " (5, 0.045439409410188321),\n",
       " (6, 0.0024892799939609366),\n",
       " (7, 0.0034500561036447455),\n",
       " (8, 0.045117513174053991),\n",
       " (9, 0.016358396992505981)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_lsi = lsi[query_vector]\n",
    "vec_lsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lsi_index = similarities.SparseMatrixSimilarity(lsi_corpus, num_features=len(lsi_corpus))\n",
    "lsi_index.save('/Users/shwld/Downloads/wiki-corpus/gensim.lsi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9, 0.76265842),\n",
       " (7, 0.70202541),\n",
       " (2, 1.5832484e-08),\n",
       " (6, 6.9849193e-09),\n",
       " (5, 5.4205884e-09),\n",
       " (0, -7.4505806e-09),\n",
       " (3, -1.0011718e-08),\n",
       " (1, -1.4668331e-08),\n",
       " (8, -1.5599653e-08),\n",
       " (4, -2.2351742e-08)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims = lsi_index[vec_lsi]\n",
    "sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "sims\n",
    "# print(sorted(enumerate(sims), key=lambda item: -item[1])[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参照元\n",
    "- http://kensuke-mi.hatenablog.com/entry/20131021/1382384297\n",
    "- gensimでcosine_similarity\n",
    "  - http://blog.yuku-t.com/entry/20110623/1308810518"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
