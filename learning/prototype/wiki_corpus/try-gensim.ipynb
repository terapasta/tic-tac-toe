{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import MeCab\n",
    "import mojimoji\n",
    "\n",
    "\n",
    "class MecabTokenizer:\n",
    "    def __init__(self):\n",
    "        self.tagger = MeCab.Tagger(\"-u dict/custom.dic\")\n",
    "        # Note: node.surfaceを取得出来るようにするため、空文字をparseする(Python3のバグの模様)\n",
    "        self.tagger.parse('')\n",
    "\n",
    "    def tokenize(self, texts):\n",
    "        splited_texts = []\n",
    "        for text in texts:\n",
    "            splited_texts.append(self.tokenize_single_text(text))\n",
    "        return splited_texts\n",
    "\n",
    "    def tokenize_single_text(self, text):\n",
    "        node = self.tagger.parseToNode(text)\n",
    "        word_list = []\n",
    "        while node:\n",
    "            features = node.feature.split(',')\n",
    "            pos = features[0]\n",
    "\n",
    "            if pos in [\"名詞\", \"動詞\", \"形容詞\", \"感動詞\", \"助動詞\", \"副詞\"]:\n",
    "                lemma = node.feature.split(\",\")[6]\n",
    "\n",
    "                if pos == '名詞' and features[1] == '非自立':\n",
    "                    node = node.next\n",
    "                    continue\n",
    "                if pos == '動詞' and features[1] == '非自立':\n",
    "                    node = node.next\n",
    "                    continue\n",
    "\n",
    "                if pos == '助動詞' and lemma != 'ない':\n",
    "                    node = node.next\n",
    "                    continue\n",
    "\n",
    "                if lemma == 'ある':\n",
    "                    node = node.next\n",
    "                    continue\n",
    "\n",
    "                if lemma == \"*\":\n",
    "                    lemma = node.surface\n",
    "\n",
    "                word_list.append(mojimoji.han_to_zen(lemma))\n",
    "            node = node.next\n",
    "        return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_table('/Users/shwld/Downloads/wiki-corpus/jawiki.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6796444,)\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_5300000-5400000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_5400000-5500000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_5500000-5600000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_5600000-5700000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_5700000-5800000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_5800000-5900000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_5900000-6000000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_6000000-6100000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_6100000-6200000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_6200000-6300000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_6300000-6400000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_6400000-6500000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_6500000-6600000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_6600000-6700000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_6700000-6800000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "print(data['question'].shape)\n",
    "\n",
    "i = 5300000\n",
    "j = 5400000\n",
    "while i < data['question'].shape[0]:\n",
    "    filename = '/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_{start}-{end}'.format(start=i, end=j)\n",
    "    print(filename)\n",
    "    tokenized_sentences = None\n",
    "    tokenized_sentences = MecabTokenizer().tokenize(data['question'][i:j])\n",
    "    joblib.dump(tokenized_sentences, filename)\n",
    "    i = i + 100000\n",
    "    j = j + 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_0-100000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_100000-200000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_200000-300000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_300000-400000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_400000-500000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_500000-600000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_600000-700000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_700000-800000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_800000-900000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_900000-1000000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_1000000-1100000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_1100000-1200000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_1200000-1300000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_1300000-1400000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_1400000-1500000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_1500000-1600000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_1600000-1700000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_1700000-1800000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_1800000-1900000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_1900000-2000000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_2000000-2100000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_2100000-2200000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_2200000-2300000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_2300000-2400000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_2400000-2500000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_2500000-2600000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_2600000-2700000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_2700000-2800000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_2800000-2900000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_2900000-3000000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_3000000-3100000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_3100000-3200000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_3200000-3300000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_3300000-3400000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_3400000-3500000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_3500000-3600000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_3600000-3700000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_3700000-3800000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_3800000-3900000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_3900000-4000000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_4000000-4100000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_4100000-4200000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_4200000-4300000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_4300000-4400000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_4400000-4500000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_4500000-4600000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_4600000-4700000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_4700000-4800000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_4800000-4900000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_4900000-5000000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_5000000-5100000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_5100000-5200000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_5200000-5300000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_5300000-5400000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_5400000-5500000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_5500000-5600000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_5600000-5700000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_5700000-5800000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_5800000-5900000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_5900000-6000000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_6000000-6100000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_6100000-6200000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_6200000-6300000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_6300000-6400000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_6400000-6500000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_6500000-6600000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_6600000-6700000\n",
      "/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_6700000-6800000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6696444"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from gensim import models\n",
    "from gensim import corpora, models, similarities\n",
    "\n",
    "i = 0\n",
    "j = 100000\n",
    "tokenized_sentences = []\n",
    "while i < data['question'].shape[0]:\n",
    "    filepath = '/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list_{start}-{end}'.format(start=i, end=j)\n",
    "    print(filepath)\n",
    "    if os.path.exists(filepath):\n",
    "        tokenized_sentences = tokenized_sentences + joblib.load(filepath)\n",
    "    i = i + 100000\n",
    "    j = j + 100000\n",
    "\n",
    "len(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# joblib.dump(tokenized_sentences, '/Users/shwld/Downloads/wiki-corpus/tokenized_sentences_list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tokenized_sentences = [\n",
    "#     ['アンパサンド', '（，', '＆）', '意味', 'する', '記号', 'ラテン語', '合', '字', 'Ｔｒｅｂｕｃｈｅｔ', 'ＭＳ', 'フォント', '表示', 'する', 'れる', '”', 'ｅｔ', '”', '合', '字', '容易', 'わかる', 'ａｍｐｅｒｓａ', '”', 'ａｎｄ', 'ｐｅｒ', 'ｓｅ', 'ａｎｄ', '”、', '意味', '”', 'ａｎｄ', '［', 'ｔｈｅ', 'ｓｙｍｂｏｌ', 'ｗｈｉｃｈ', '］', 'ｂｙ', 'ｉｔｓｅｌｆ', '［', 'ｉｓ', '］', 'ａｎｄ', '”'],\n",
    "#     ['使用', '１', '世紀', '遡る', 'できる', '（', '１', '）、', '５', '世紀', '中葉', '（', '２', '，', '３', '）', '現代', '（', '４', '−', '６', '）', '至る', '変遷', 'わかる'],\n",
    "#     ['Ｚ', '続く', 'ラテン', '文字', 'アルファベット', '２７', '字', '目', 'する', 'れる', '時期'],\n",
    "#     ['アンパサンド', '役割', '果たす', '文字', 'ｅｔ', '呼ぶ', 'れる', '数字', '７', '似る', '記号', '（，', 'Ｕ', '＋', '２０４', 'Ａ', '）。', '記号', '現在', 'ゲール', '文字', '使う', 'れる'],\n",
    "#     ['記号', '名', 'アンパサンド', 'ラテン語', 'まじる', '英語', '「＆', 'それ', '自身', '”', 'ａｎｄ', '”', '表す', '（＆', 'ｐｅｒ', 'ｓｅ', 'ａｎｄ', '）', 'くずれる', '形', '英語', '言語', '名称', '多様'],\n",
    "#     ['日常', '的', '手書き', '場合', '欧米', 'アンパサンド', '縦', '線', '引く', '単純', '化', 'する', 'れる', '使う', 'れる'],\n",
    "#     ['同様', 'ｔ', '「＋（', 'プラス', '輪', '重ねる', '無声', '歯茎', '側面', '摩擦音', '示す', '発音', '記号', '使う', 'れる'],\n",
    "#     ['プログラミング', '言語', 'Ｃ', '多数', '言語', 'ＡＮＤ', '演算', '子', '用いる', 'られる', 'Ｃ', '例'],\n",
    "#     ['ＰＨＰ', '変数', '宣言', '記号', '（＄）', '直前', '記述', 'する', '参照', '渡し', '行う', 'できる'],\n",
    "#     ['ＢＡＳＩＣ', '系列', '言語', '文字', '列', '連結', '演算', '子', '使用', 'する', 'れる', 'ｃｏｄｉｃｅ', '＿', '４', 'ｃｏｄｉｃｅ', '＿', '５', '返す', '主', 'マイクロソフト', '系', '整数', '十', '六', '進', '表記', 'ｃｏｄｉｃｅ', '＿', '６', '用いる', 'ｃｏｄｉｃｅ', '＿', '７', '十', '進', '１５', '表現', 'する']\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gensim_dictionary=corpora.Dictionary(tokenized_sentences)\n",
    "gensim_dictionary.save('/Users/shwld/Downloads/wiki-corpus/gensim.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#corpusをid表示に変換する．二次元リストで構築しているので内包表記で記述\n",
    "id_corpus=[gensim_dictionary.doc2bow(sentence) for sentence in tokenized_sentences];\n",
    "#保存しておく \n",
    "corpora.MmCorpus.serialize('/Users/shwld/Downloads/wiki-corpus/gensim.mm', id_corpus);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tfidfモデルの構築とcorpusの変換\n",
    "tfidf=models.TfidfModel(id_corpus)\n",
    "tfidf.save('/Users/shwld/Downloads/wiki-corpus/gensim.tfidf')\n",
    "tfidf_corpus=tfidf[id_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#lsiモデルの構築とcorpusの変換\n",
    "lsi=models.LsiModel(corpus=tfidf_corpus, num_topics=10, id2word=gensim_dictionary)\n",
    "lsi.save('/Users/shwld/Downloads/wiki-corpus/gensim.lsi')\n",
    "lsi_corpus=lsi[tfidf_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(lsi_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "queries = ['マイクロソフト', '演算']\n",
    "query_vector = gensim_dictionary.doc2bow(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vec_lsi = lsi[query_vector]\n",
    "vec_lsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lsi_index = similarities.SparseMatrixSimilarity(lsi_corpus, num_features=len(lsi_corpus))\n",
    "lsi_index.save('/Users/shwld/Downloads/wiki-corpus/gensim.lsi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sims = lsi_index[vec_lsi]\n",
    "sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "sims\n",
    "# print(sorted(enumerate(sims), key=lambda item: -item[1])[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参照元\n",
    "- http://kensuke-mi.hatenablog.com/entry/20131021/1382384297\n",
    "- gensimでcosine_similarity\n",
    "  - http://blog.yuku-t.com/entry/20110623/1308810518"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
